# 2026-02-04 - LTX-2 Video Generation Setup

## Session Summary
Debugging session for LTX-2 local video generation on RTX 5090 (32GB VRAM).

## Key Findings

### Model Inventory (Available Models)
| Model | Location | Size | Status |
|-------|----------|------|--------|
| ltx-2-19b-dev-fp8.safetensors | /mnt/c/models/diffusion_models/ | 26GB | ✅ Available |
| ltx-video-2b-v0.9.1.safetensors | /mnt/c/models/diffusion_models/ | 5.4GB | ✅ Available |
| qwen_2.5_vl_7b_fp8_scaled.safetensors | /mnt/c/models/text_encoders/ | 8.8GB | ✅ Available |
| ltx-2-spatial-upscaler-x2-1.0.safetensors | /mnt/c/models/upscale_models/ | ~2GB | ✅ Available |

### Missing Models (Required by ComfyUI Template)
- `ltx-2-19b-distilled.safetensors` (~38GB) - NOT present
- `gemma_3_12B_it_fp4_mixed.safetensors` (~6GB) - NOT present

## Technical Issues Encountered

### 1. Meta Tensor Error
**Error:** `RuntimeError: Cannot copy out of meta tensor; no data!`
**Fix:** Modified `single_gpu_model_builder.py` to use `to_empty()` instead of direct copy.

### 2. Dtype Mismatch Error  
**Error:** `RuntimeError: expected m1 and m2 to have the same dtype, but got: float != c10::BFloat16`
**Fix:** Modified `feature_extractor.py` to cast input to layer dtype.

### 3. Device Mismatch Error
**Error:** `RuntimeError: Expected all tensors to be on the same device, but got mat1 is on cpu`
**Fix:** Modified `transformer_args.py` to move context to projection device.

### 4. CUDA OOM (Critical Blocker)
**Error:** `CUDA out of memory` - 27.73 GiB allocated, only 49 MiB free
**Root Cause:** Two-stage pipeline tries to load Stage 2 transformer while Stage 1 still in memory.
**Attempted Fix:** Explicit cleanup between stages + `torch.cuda.empty_cache()` - still failing.

## Modified Files (LTX-2 Package)
All changes in `/tmp/LTX-2/` (editable install):
1. `packages/ltx-core/src/ltx_core/loader/single_gpu_model_builder.py`
2. `packages/ltx-pipelines/src/ltx_pipelines/utils/model_ledger.py`
3. `packages/ltx-core/src/ltx_core/text_encoders/gemma/feature_extractor.py`
4. `packages/ltx-core/src/ltx_core/model/transformer/transformer_args.py`
5. `packages/ltx-pipelines/src/ltx_pipelines/ti2vid_two_stages.py`

## Successful Outputs Exist
User already has working ComfyUI outputs:
- `~/ComfyUI/output/ltxv_robot.mp4`
- `~/ComfyUI/output/ltxv_first_i2v_video.webp`
- `~/ComfyUI/output/ltxv_t2v_test_00001_.webm`

## Next Steps (Pending)
1. User wants funny example videos once working
2. Options discussed:
   - Use ComfyUI directly (recommended - already works)
   - Download missing distilled model + Gemma encoder
   - Further debug two-stage pipeline memory issues

## User Preference
User indicated ComfyUI works but wants automated/scripted generation. Open to either downloading correct models or debugging current setup further.

## Resource Constraints
- RTX 5090 with 32GB VRAM
- System RAM hitting 100% during attempts
- Need to stay within VRAM budget for both stages
